{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University of Stirling\n",
    "\n",
    "# ITNPBD2 Representing and Manipulating Data\n",
    "\n",
    "# Assignment Autumn 2025\n",
    "\n",
    "# A Consultancy Job for JC Penney\n",
    "\n",
    "This notebook forms the assignment instructions and submission document of the assignment for ITNPBD2. Read the instructions carefully and enter code into the cells as indicated.\n",
    "\n",
    "You will need these five files, which were in the Zip file you downloaded from the course webpage:\n",
    "\n",
    "- jcpenney_reviewers.json\n",
    "- jcpenney_products.json\n",
    "- products.csv\n",
    "- reviews.csv\n",
    "- users.csv\n",
    "\n",
    "The data in these files describes products that have been sold by the American retail giant, JC Penney, and reviews by customers who bought them. Note that the product data is real, but the customer data is synthetic.\n",
    "\n",
    "Your job is to process the data, as requested in the instructions in the markdown cells in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completing the Assignment\n",
    "\n",
    "Rename this file to be xxxxxx_BD2 where xxxxxx is your student number, then type your code and narrative description into the boxes provided. Add as many code and markdown cells as you need. The cells should contain:\n",
    "\n",
    "- **Text narrative describing what you did with the data**\n",
    "- **The code that performs the task you have described**\n",
    "- **Comments that explain your code**\n",
    "\n",
    "The final structure (in PDF) of your report must:\n",
    "- **Start from the main insights observed (max 5 pages)**\n",
    "- **Include as an appendix the source code used for producing those insights (max 15 pages)**\n",
    "- **Include an AI cover sheet (provided on Canvas), which must contain a link to a versioned notebook file in OneDrive or another platform for version checks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marking Scheme\n",
    "The assessment will be marked against the university Common Marking Scheme (CMS)\n",
    "\n",
    "Here is a summary of what you need to achieve to gain a grade in the major grade bands:\n",
    "\n",
    "|Grade|Requirement|\n",
    "|:---|:---|\n",
    "| Fail | You will fail if your code does not run or does not achieve even the basics of the task. You may also fail if you submit code without either comments or a text explanation of what the code does.|\n",
    "| Pass | To pass, you must submit sufficient working code to show that you have mastered the basics of the task, even if not everything works completely. You must include some justifications for your choice of methods, but without mentioning alternatives. |\n",
    "| Merit | For a merit, your code must be mostly correct, with only small problems or parts missing, and your comments must be useful rather than simply re-stating the code in English. Most choices for methods and structures should be explained and alternatives mentioned. |\n",
    "| Distinction | For a distinction, your code must be working, correct, and well commented and shows an appreciation of style, efficiency and reliability. All choices for methods and structures are concisely justified and alternatives are given well thought considerations. For a distinction, your work should be good enough to present to executives at the company.|\n",
    "\n",
    "The full details of the CMS can be found here\n",
    "\n",
    "https://www.stir.ac.uk/about/professional-services/student-academic-and-corporate-services/academic-registry/academic-policy-and-practice/quality-handbook/assessment-policy-and-procedure/appendix-2-postgraduate-common-marking-scheme/\n",
    "\n",
    "Note that this means there are not certain numbers of marks allocated to each stage of the assignment. Your grade will reflect how well your solutions and comments demonstrate that you have achieved the learning outcomes of the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "When you are ready to submit, **print** your notebook as PDF (go to File -> Print Preview) in the Jupyter menu. Make sure you have run all the cells and that their output is displayed. Any lines of code or comments that are not visible in the pdf should be broken across several lines. You can then submit the file online.\n",
    "\n",
    "Late penalties will apply at a rate of three marks per day, up to a maximum of 7 days. After 7 days you will be given a mark of 0. Extensions will be considered under acceptable circumstances outside your control.\n",
    "\n",
    "## Academic Integrity\n",
    "\n",
    "This is an individual assignment, and so all submitted work must be fully your own work.\n",
    "\n",
    "The University of Stirling is committed to protecting the quality and standards of its awards. Consequently, the University seeks to promote and nurture academic integrity, support staff academic integrity, and support students to understand and develop good academic skills that facilitate academic integrity.\n",
    "\n",
    "In addition, the University deals decisively with all forms of Academic Misconduct.\n",
    "\n",
    "Where a student does not act with academic integrity, their work or behaviour may demonstrate Poor Academic Practice or it may represent Academic Misconduct.\n",
    "\n",
    "### Poor Academic Practice\n",
    "\n",
    "Poor Academic Practice is defined as: \"The submission of any type of assessment with a lack of referencing or inadequate referencing which does not effectively acknowledge the origin of words, ideas, images, tables, diagrams, maps, code, sound and any other sources used in the assessment.\"\n",
    "\n",
    "### Academic Misconduct\n",
    "\n",
    "Academic Misconduct is defined as: \"any act or attempted act that does not demonstrate academic integrity and that may result in creating an unfair academic advantage for you or another person, or an academic disadvantage for any other member or member of the academic community.\"\n",
    "\n",
    "Plagiarism is presenting somebody else’s work as your own **and includes the use of artificial intelligence tools beyond AIAS Level 2 or the use of Large Language Models.**. Plagiarism is a form of academic misconduct and is taken very seriously by the University. Students found to have plagiarised work can have marks deducted and, in serious cases, even be expelled from the University. Do not submit any work that is not entirely your own. Do not collaborate with or get help from anybody else with this assignment.\n",
    "\n",
    "The University of Stirling's full policy on Academic Integrity can be found at:\n",
    "\n",
    "https://www.stir.ac.uk/about/professional-services/student-academic-and-corporate-services/academic-registry/academic-policy-and-practice/quality-handbook/academic-integrity-policy-and-academic-misconduct-procedure/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Assignment\n",
    "Your task with this assignment is to use the data provided to demonstrate your Python data manipulation skills.\n",
    "\n",
    "There are three `.csv` files and two `.json` files so you can process different types of data. The files also contain unstructured data in the form of natural language in English and links to images that you can access from the JC Penney website (use the field called `product_image_urls`).\n",
    "\n",
    "Start with easy tasks to show you can read in a file, create some variables and data structures, and manipulate their contents. Then move onto something more interesting.\n",
    "\n",
    "Look at the data that we provided with this assessment and think of something interesting to do with it using whatever libraries you like. Describe what you decide to do with the data and why it might be interesting or useful to the company to do it.\n",
    "\n",
    "You can add additional data if you need to - either download it or access it using `requests`. Produce working code to implement your ideas in as many cells as you need below. There is no single right answer, the aim is to simply show you are competent in using python for data analysis. Exactly how you do that is up to you.\n",
    "\n",
    "For a distinction class grade, this must show originality, creative thinking, and insights beyond what you've been taught directly on the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "You may structure the appendix of the project how you wish, but here is a suggested guideline to help you organise your work, based on the CRISP-DM data science methodology:\n",
    "\n",
    " 1. **Business understanding** - What business context is the data coming from? What insights would be valuable in that context, and what data would be required for that purporse? \n",
    " 2. **Data understanding and preparation** - Explore the data and show you understand its structure and relations, with the aid of appropriate visualisation techniques. Assess the data quality, which insights you would be able to answer from it, and what preparation the data would require. Add new data from another source if required to bring new insights to the data you already have.\n",
    " 3. **Data modeling (optional)** - Would modeling be required for the insights you have considered? Use appropriate techniques, if so.\n",
    " 4. **Evaluation and deployment** - How do the insights you obtained help the company, and how can should they be adopted in their business? If modeling techniques have been adopted, are their use scientifically sound and how should they be mantained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember to make sure you are working completely on your own.\n",
    "# Don't work in a group or with a friend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **JCPenny Consultancy Analysis**\n",
    "### **Date: 27/10/2025**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup Instructions\n",
    "\n",
    "## Setting up the Environment with Anaconda\n",
    "\n",
    "Follow these steps to set up your environment for running this Jupyter notebook:\n",
    "\n",
    "### 1. Clone the Repository\n",
    "```bash\n",
    "git clone https://github.com/yakubuaisha318-gif/Representation_and_Manipulation_of_Data_JC_Penny_Consultancy_Assignment.git\n",
    "cd Representation_and_Manipulation_of_Data_JC_Penny_Consultancy_Assignment\n",
    "```\n",
    "\n",
    "### 2. Install Anaconda\n",
    "If you haven't already installed Anaconda, download it from [anaconda.com](https://www.anaconda.com/products/distribution) and follow the installation instructions for your operating system.\n",
    "\n",
    "### 3. Create a New Conda Environment\n",
    "```bash\n",
    "conda create -n jcpenney-analysis python=3.9\n",
    "```\n",
    "\n",
    "### 4. Activate the Environment\n",
    "```bash\n",
    "conda activate jcpenney-analysis\n",
    "```\n",
    "\n",
    "### 5. Install Required Dependencies\n",
    "```bash\n",
    "pip install -r requrements.txt\n",
    "```\n",
    "\n",
    "If the requirements file is not available, install the necessary packages:\n",
    "```bash\n",
    "conda install pandas numpy matplotlib openpyxl\n",
    "pip install fpdf\n",
    "```\n",
    "\n",
    "### 6. Start Jupyter Notebook\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### 7. Open and Run This Notebook\n",
    "1. Navigate to this notebook file in the Jupyter interface\n",
    "2. Select the kernel: `Kernel` → `Change kernel` → `jcpenney-analysis`\n",
    "3. Run the cells: `Cell` → `Run All`\n",
    "\n",
    "### 8. Deactivating the Environment\n",
    "When you're done working:\n",
    "```bash\n",
    "conda deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converts JSON file to JSON array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jcpenney Products =  7982\n",
      "Total jcpenney Reviewers =  5000\n"
     ]
    }
   ],
   "source": [
    "def convert_json_file_to_json_array(json_file_path: str) -> List[Dict[str, Any]]:\n",
    "    '''Convert a JSON Lines file to a JSON array.\n",
    "    Args:\n",
    "        json_file_path (str): The path to the JSON Lines file.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries representing the JSON array.'''\n",
    "    data: List[Dict[str, Any]] = []\n",
    "    try:\n",
    "        with open(json_file_path, \"r\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except:\n",
    "                        continue\n",
    "    except:\n",
    "        pass\n",
    "    return data\n",
    "jcpenney_products = convert_json_file_to_json_array(\"jcpenney_products.json\")\n",
    "jcpenney_reviewers = convert_json_file_to_json_array(\"jcpenney_reviewers.json\")\n",
    "\n",
    "total_jcpenney_products = len(jcpenney_products)\n",
    "total_jcpenney_reviewers = len(jcpenney_reviewers)\n",
    "\n",
    "print(\"Total jcpenney Products = \",total_jcpenney_products)\n",
    "print(\"Total jcpenney Reviewers = \",total_jcpenney_reviewers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converts CSV files to JSON arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Products =  7982\n",
      "Total Reviews =  39063\n",
      "Total Users =  5000\n"
     ]
    }
   ],
   "source": [
    "def convert_csv_file_to_json_array(csv_file_path: str) -> List[Any]:\n",
    "    '''Convert a CSV file to a JSON array.\n",
    "    Args:\n",
    "        csv_file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        List[Any]: A list of dictionaries representing the JSON array.'''\n",
    "    try:\n",
    "        df: pd.DataFrame = pd.read_csv(csv_file_path)\n",
    "        json_str = df.to_json(orient=\"records\")\n",
    "        return json.loads(json_str) if json_str else []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "products = convert_csv_file_to_json_array(\"products.csv\")\n",
    "reviews = convert_csv_file_to_json_array(\"reviews.csv\")\n",
    "users = convert_csv_file_to_json_array(\"users.csv\")\n",
    "\n",
    "total_products = len(products)\n",
    "total_reviews = len(reviews)\n",
    "total_users = len(users)\n",
    "\n",
    "print(\"Total Products = \",total_products)\n",
    "print(\"Total Reviews = \",total_reviews)\n",
    "print(\"Total Users = \",total_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Comparison & Validation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon a critical examination of the data, it was observed that jcpenney_products.json is a more detailed version of products.csv, and jcpenney_reviewers.json is a more detailed version of users.csv. Consequently, data comparison and validation functions were developed to analyze the relationships between these files.\n",
    "\n",
    "Also, upon further investigation i realized there was a duplicate username(dqft3311) however, different date of birth and states hence not duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviewers Dataset Comparison:\n",
      "  JCPenney data contains all fields from CSV (with different names):\n",
      "  Truly additional fields in JCPenney data: 1\n",
      "    - Reviewed\n",
      "\n",
      "Products Dataset Comparison:\n",
      "  JCPenney data contains all fields from CSV (with different names):\n",
      "    - SKU -> sku\n",
      "    - Uniq_id -> uniq_id\n",
      "    - Name -> name_title\n",
      "    - Price -> list_price\n",
      "    - Description -> description\n",
      "    - Av_Score -> average_product_rating\n",
      "  Truly additional fields in JCPenney data: 9\n",
      "    - Bought With\n",
      "    - Reviews\n",
      "    - brand\n",
      "    - category\n",
      "    - category_tree\n",
      "    - product_image_urls\n",
      "    - product_url\n",
      "    - sale_price\n",
      "    - total_number_reviews\n",
      "len of reviewers (json file)  5000\n",
      "len of users (csv file)  5000\n",
      "Using jcpenney_reviewers for processing (contains additional fields)\n",
      "len of jcpenny products (json file)  7982\n",
      "len of products (csv file)  7982\n",
      "Using jcpenney_products for processing (contains additional fields)\n"
     ]
    }
   ],
   "source": [
    "def compare_dataset_fields(jcpenney_data: List[Dict[str, Any]], csv_data: List[Dict[str, Any]], \n",
    "                          dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Compare fields between JCPenney dataset and CSV dataset to identify additional fields.\n",
    "    \n",
    "    Args:\n",
    "        jcpenney_data: The JCPenney dataset (richer version)\n",
    "        csv_data: The CSV dataset (simpler version)\n",
    "        dataset_name: Name of the dataset for reporting purposes\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing lists of additional fields in JCPenney data and missing fields\n",
    "    \"\"\"\n",
    "    if not jcpenney_data or not csv_data:\n",
    "        return {\"additional_fields\": [], \"missing_fields\": [], \"field_mappings\": {}}\n",
    "    \n",
    "    # Get field sets from both datasets\n",
    "    jcpenney_fields = set(jcpenney_data[0].keys()) if jcpenney_data else set()\n",
    "    csv_fields = set(csv_data[0].keys()) if csv_data else set()\n",
    "    \n",
    "    # Define field mappings between CSV and JCPenney datasets\n",
    "    field_mappings = {\n",
    "        \"SKU\": \"sku\",\n",
    "        \"Uniq_id\": \"uniq_id\",\n",
    "        \"Name\": \"name_title\",\n",
    "        \"Price\": \"list_price\",\n",
    "        \"Description\": \"description\",\n",
    "        \"Av_Score\": \"average_product_rating\"\n",
    "    }\n",
    "    \n",
    "    # Reverse mapping for easier lookup\n",
    "    reverse_mappings = {v: k for k, v in field_mappings.items()}\n",
    "    \n",
    "    # Identify truly additional fields (not just renamed)\n",
    "    additional_fields = []\n",
    "    missing_fields = []\n",
    "    \n",
    "    for field in jcpenney_fields:\n",
    "        # Check if this is a renamed version of a CSV field\n",
    "        if field not in field_mappings.values() or reverse_mappings.get(field) not in csv_fields:\n",
    "            # Not a simple renaming, so it's truly additional\n",
    "            if field not in [reverse_mappings.get(f, f) for f in csv_fields]:\n",
    "                additional_fields.append(field)\n",
    "    \n",
    "    for field in csv_fields:\n",
    "        # Check if this field is missing (not just renamed)\n",
    "        if field not in field_mappings or field_mappings[field] not in jcpenney_fields:\n",
    "            # Not a simple renaming, so it's truly missing\n",
    "            if field not in [field_mappings.get(f, f) for f in jcpenney_fields]:\n",
    "                missing_fields.append(field)\n",
    "    \n",
    "    # Get the actual mappings that exist\n",
    "    actual_mappings = {}\n",
    "    for csv_field, jcp_field in field_mappings.items():\n",
    "        if csv_field in csv_fields and jcp_field in jcpenney_fields:\n",
    "            actual_mappings[csv_field] = jcp_field\n",
    "    \n",
    "    return {\n",
    "        \"additional_fields\": sorted(additional_fields),\n",
    "        \"missing_fields\": sorted(missing_fields),\n",
    "        \"field_mappings\": actual_mappings\n",
    "    }\n",
    "\n",
    "def print_dataset_comparison(jcpenney_data: List[Dict[str, Any]], csv_data: List[Dict[str, Any]], \n",
    "                           dataset_name: str) -> None:\n",
    "    \"\"\"Print a comparison of fields between datasets.\n",
    "    \n",
    "    Args:\n",
    "        jcpenney_data: The JCPenney dataset (richer version)\n",
    "        csv_data: The CSV dataset (simpler version)\n",
    "        dataset_name: Name of the dataset for reporting purposes\n",
    "    \"\"\"\n",
    "    comparison = compare_dataset_fields(jcpenney_data, csv_data, dataset_name)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Dataset Comparison:\")\n",
    "    print(f\"  JCPenney data contains all fields from CSV (with different names):\")\n",
    "    for csv_field, jcp_field in comparison['field_mappings'].items():\n",
    "        print(f\"    - {csv_field} -> {jcp_field}\")\n",
    "    \n",
    "    if comparison['additional_fields']:\n",
    "        print(f\"  Truly additional fields in JCPenney data: {len(comparison['additional_fields'])}\")\n",
    "        for field in comparison['additional_fields']:\n",
    "            print(f\"    - {field}\")\n",
    "    \n",
    "    if comparison['missing_fields']:\n",
    "        print(f\"  Fields missing in JCPenney data: {len(comparison['missing_fields'])}\")\n",
    "        for field in comparison['missing_fields']:\n",
    "            print(f\"    - {field}\")\n",
    "\n",
    "# Compare datasets and identify additional fields\n",
    "print_dataset_comparison(jcpenney_reviewers, users, \"Reviewers\")\n",
    "print_dataset_comparison(jcpenney_products, products, \"Products\")\n",
    "\n",
    "jcpenney_reviewers_usernames = [reviewer[\"Username\"] for reviewer in jcpenney_reviewers]\n",
    "users_usernames = [user[\"Username\"] for user in users]\n",
    "\n",
    "jcpenney_products_uniq_ids = [product[\"uniq_id\"] for product in jcpenney_products]\n",
    "products_uniq_ids = [product[\"Uniq_id\"] for product in products]\n",
    "\n",
    "print(\"len of reviewers (json file) \", len(jcpenney_reviewers))\n",
    "print(\"len of users (csv file) \", len(users))\n",
    "# compare length and unique elements of reviewers and users\n",
    "if (set(jcpenney_reviewers_usernames).issubset(set(users_usernames)) and \n",
    "    len(jcpenney_reviewers_usernames) == len(users_usernames)):\n",
    "    selected_reviewers = jcpenney_reviewers  # Use the richer dataset\n",
    "    print(\"Using jcpenney_reviewers for processing (contains additional fields)\")\n",
    "else:\n",
    "    selected_reviewers = users  # Fallback to simpler dataset\n",
    "print(\"len of jcpenny products (json file) \", len(jcpenney_products))\n",
    "print(\"len of products (csv file) \", len(products))\n",
    "# compare length and unique elements of products and jcpenney_products\n",
    "if (set(jcpenney_products_uniq_ids).issubset(set(products_uniq_ids)) and \n",
    "    len(jcpenney_products_uniq_ids) == len(products_uniq_ids)):\n",
    "    selected_products = jcpenney_products  # Use the richer dataset\n",
    "    print(\"Using jcpenney_products for processing (contains additional fields)\")\n",
    "else:\n",
    "    selected_products = products  # Fallback to simpler dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first four float extracted from a scored field in reviews using extract value function [2.0, 1.0, 2.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_values(data: List[Dict[str, Any]], field_name: str) -> List[float]:\n",
    "    \"\"\"Extract numeric values from a field in a list of dictionaries.\n",
    "    Args:\n",
    "        data (List[Dict[str, Any]]): The list of dictionaries containing the data.\n",
    "        field_name (str): The name of the field to extract values from.\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: A list of extracted numeric values converted to floats.\"\"\"\n",
    "    values = []\n",
    "    for item in data:\n",
    "        value_str = item.get(field_name)\n",
    "        if value_str is not None:\n",
    "            try:\n",
    "                values.append(float(value_str))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    return values\n",
    "\n",
    "# Printing first value I extract\n",
    "print('first four float extracted from a scored field in reviews using extract value function',extract_values(reviews, \"Score\")[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 7982, 'mean': 2.9886828222275152, 'median': 3.0, 'std': 0.9116162637616239, 'min': 1.0, 'max': 5.0}\n"
     ]
    }
   ],
   "source": [
    "def analyze_numeric_data(products_data: List[Dict[str, Any]], field_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze numeric data across all products\n",
    "    Args:\n",
    "        products_data (List[Dict[str, Any]]): The list of dictionaries containing the products data.\n",
    "        field_name (str): The name of the field to extract numeric values from.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the count, mean, median, standard deviation, minimum, and maximum values.\"\"\"\n",
    "    values = extract_values(products_data, field_name)\n",
    "    if not values:\n",
    "        return {}\n",
    "    \n",
    "    return {\n",
    "        \"count\": len(values),\n",
    "        \"mean\": np.mean(values),\n",
    "        \"median\": np.median(values),\n",
    "        \"std\": np.std(values),\n",
    "        \"min\": np.min(values),\n",
    "        \"max\": np.max(values)\n",
    "    }\n",
    "\n",
    "print(analyze_numeric_data(jcpenney_products, \"average_product_rating\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top rated products:  ['Danny & Nicole® Sleeveless Printed Fit-and-Flare Dress - Plus', 'Danny & Nicole® Sleeveless Printed Fit-and-Flare Dress - Plus', 'Danny & Nicole® Sleeveless Striped Colorblock Fit-and-Flare Dress', 'Azul by Maxine of Hollywood Tankini Swim Top or Skirted Bottoms', 'Azzure 2-Pack Decorative Pillows']\n"
     ]
    }
   ],
   "source": [
    "def get_products(products_data: List[Dict[str, Any]], sort_field: str, top_n: int = 5, highest: bool = True) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get top or bottom products based on a specified field.\n",
    "    Args:\n",
    "        products_data (List[Dict[str, Any]]): The list of dictionaries containing the products data.\n",
    "        sort_field (str): The name of the field to sort the products by.\n",
    "        top_n (int, optional): The number of top or bottom products to retrieve. Defaults to 5.\n",
    "        highest (bool, optional): Whether to retrieve the highest or lowest values. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries representing the top or bottom products.\"\"\"\n",
    "    valid_products = [p for p in products_data if p.get(sort_field) is not None]\n",
    "    sorted_products = sorted(valid_products, \n",
    "                           key=lambda x: float(x[sort_field]), \n",
    "                           reverse=highest)\n",
    "    return sorted_products[:top_n]\n",
    "    \n",
    "top_rated_products = get_products(jcpenney_products, \"average_product_rating\", 5, True)\n",
    "print(\"Top rated products: \", [product.get('name_title') for product in top_rated_products])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By User demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_reviewers': 5000,\n",
       " 'state_distribution': {'Oregon': 96,\n",
       "  'Massachusetts': 107,\n",
       "  'Idaho': 79,\n",
       "  'Florida': 89,\n",
       "  'Georgia': 79,\n",
       "  'Montana': 97,\n",
       "  'Pennsylvania': 86,\n",
       "  'Connecticut': 82,\n",
       "  'Arkansas': 92,\n",
       "  'Nebraska': 90,\n",
       "  'California': 99,\n",
       "  'New Hampshire': 83,\n",
       "  'District of Columbia': 83,\n",
       "  'Washington': 94,\n",
       "  'Minnesota': 77,\n",
       "  'New Mexico': 96,\n",
       "  'Virginia': 96,\n",
       "  'Kansas': 90,\n",
       "  'Illinois': 69,\n",
       "  'North Dakota': 85,\n",
       "  'Colorado': 85,\n",
       "  'New York': 83,\n",
       "  'Minor Outlying Islands': 92,\n",
       "  'Northern Mariana Islands': 102,\n",
       "  'West Virginia': 80,\n",
       "  'Texas': 83,\n",
       "  'South Dakota': 79,\n",
       "  'Maryland': 77,\n",
       "  'Maine': 94,\n",
       "  'Ohio': 81,\n",
       "  'Rhode Island': 93,\n",
       "  'Michigan': 76,\n",
       "  'Alaska': 94,\n",
       "  'Iowa': 94,\n",
       "  'Oklahoma': 100,\n",
       "  'Mississippi': 94,\n",
       "  'South Carolina': 77,\n",
       "  'Missouri': 84,\n",
       "  'New Jersey': 101,\n",
       "  'Tennessee': 89,\n",
       "  'North Carolina': 68,\n",
       "  'Guam': 73,\n",
       "  'Wyoming': 86,\n",
       "  'Delaware': 106,\n",
       "  'Vermont': 103,\n",
       "  'Indiana': 86,\n",
       "  'Louisiana': 80,\n",
       "  'Wisconsin': 84,\n",
       "  'Hawaii': 88,\n",
       "  'Puerto Rico': 83,\n",
       "  'Alabama': 95,\n",
       "  'Kentucky': 99,\n",
       "  'Arizona': 71,\n",
       "  'Nevada': 90,\n",
       "  'Utah': 80,\n",
       "  'American Samoa': 86,\n",
       "  'U.S. Virgin Islands': 95},\n",
       " 'top_states': [('Massachusetts', 107),\n",
       "  ('Delaware', 106),\n",
       "  ('Vermont', 103),\n",
       "  ('Northern Mariana Islands', 102),\n",
       "  ('New Jersey', 101),\n",
       "  ('Oklahoma', 100),\n",
       "  ('California', 99),\n",
       "  ('Kentucky', 99),\n",
       "  ('Montana', 97),\n",
       "  ('Oregon', 96)],\n",
       " 'age_statistics': {'count': 5000, 'mean_age': 49.7612, 'median_age': 50.0}}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_user_demographics(reviewers_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze user demographics from reviewers data.\n",
    "    Args:\n",
    "        reviewers_data (List[Dict[str, Any]]): The list of dictionaries containing the reviewers data.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the total number of reviewers, state distribution, top states, and age statistics.\"\"\"\n",
    "    states = [reviewer.get(\"State\") for reviewer in reviewers_data if reviewer.get(\"State\")]\n",
    "    dobs = [reviewer.get(\"DOB\") for reviewer in reviewers_data if reviewer.get(\"DOB\")]\n",
    "    \n",
    "    birth_years = []\n",
    "    for dob in dobs:\n",
    "        if dob:\n",
    "            try:\n",
    "                year = int(dob.split(\".\")[-1])\n",
    "                birth_years.append(year)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    current_year = 2025\n",
    "    ages = [current_year - year for year in birth_years if 1900 <= year <= current_year]\n",
    "    \n",
    "    return {\n",
    "        \"total_reviewers\": len(reviewers_data),\n",
    "        \"state_distribution\": dict(Counter(states)),\n",
    "        \"top_states\": Counter(states).most_common(10),\n",
    "        \"age_statistics\": {\n",
    "            \"count\": len(ages),\n",
    "            \"mean_age\": np.mean(ages) if ages else 0,\n",
    "            \"median_age\": np.median(ages) if ages else 0\n",
    "        } if ages else {}\n",
    "    }\n",
    "get_user_demographics(jcpenny_users)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
